% $Date: 2022/12/30 17:24:03 $
% This template file is public domain.
%
% TUGboat class documentation is at:
%   http://mirrors.ctan.org/macros/latex/contrib/tugboat/ltubguid.pdf
% or
%   texdoc tugboat

\documentclass[final]{ltugboat}
\usepackage[T1]{fontenc} % tt \_
\pdfsuppresswarningpagegroup=1
\def\ARQMath{\acro{ARQM}ath}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage[hidelinks,pdfa]{hyperref}
\usepackage{hologo}
\usepackage{ragged2e}
\makeatletter
%listings float without package
\newenvironment{listing*}{\@dblfloat{listing}}{\end@dblfloat}
\def\ext@listing{lol}
\newcommand*{\ftype@listing}{4}
\newcounter{listing}
\newcommand*{\fnum@listing}{\textbf{Listing~\thelisting}}
\def\fps@listing{p}
\makeatother
\usepackage{fancyvrb,fvextra}
\fvset{breaklines}
\usepackage{tikz}
\usetikzlibrary{calc,decorations.pathreplacing,shapes}
\usepackage{./examples}
\def\url{\tbsurl}
\def\xAllTeX{(\La\kern-.075em)\kern-.075em\TeX} % i prefer our spacing :)
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}
\renewcommand{\arraystretch}{1.3}

% silence font warnings:
% LaTeX Font Warning: Font shape `OT1/cmr/m/n' in size <45> not available
% (Font)              size <24.88> substituted on input line 94.
\makeatletter\def\@font@warning#1{}\makeatother

%%% Start of metadata %%%
\title{Fast regression testing of \TeX{} packages: Multiprocessing and~batching}

% repeat info for each author; comment out items that don't apply.
\author{V\'\i{}t Star\'y Novotn\'y}
\address{Studen\'a 453/15 \\ Brno 63800, Czech Republic}
\netaddress{witiko (at) mail dot muni dot cz}
\personalURL{github.com/witiko}

\author{Marei Peischl}
\address{Gneisenaustr. 18 \\ Hamburg 20253, Germany}
\netaddress{marei (at) peitex dot de}
\personalURL{peitex.de}

%%% End of metadata %%%

\begin{document}
\maketitle

\begin{abstract}
In the version 3.0.0 of the Markdown package for \TeX, the number of regression tests increased from 143 to 783. This caused the tests to run for up to 15~hours, which slowed down our development cycle.
In this article, we describe a novel technique for batching test files that reduced our testing time from 15 hours to just 15 minutes. With batching, the amount of time that is spent on actual testing increased from 5\% to 97\%. When combined with multiprocessing on 32 \acro{CPU}s, our batching technique achieved a speed increase of up to 161 times compared to running without any multiprocessing or batching.
\end{abstract}

\section{Introduction}
Small \TeX{} packages, typically developed in a single iteration rather than through ongoing updates, can depend on user feedback to maintain the code. However, this approach has its limitations. Larger projects, especially those that are continuously developed, require a more robust solution. Automated regression tests are crucial in these cases. They ensure that any changes, either in the code itself or its external dependencies, do not alter the expected behavior of the code.

The Markdown package for \TeX{} also features a~set of regression tests. These tests, designed to be completed in just a few minutes, provide immediate feedback and are automatically conducted on any updates submitted to the package's GitHub repository.
\looseness=-1

After the implementation of the CommonMark standard in version 3.0.0 of the Markdown package, the number of tests increased from 143 to 783 (about a 5.5-fold increase). This caused the tests to take up to 15 hours to run, using free GitHub-hosted runners, too slow to provide any benefit to developers.

In order to increase the testing speed, we implemented a novel technique for batching test files and we added self-hosted runners with up to 12 \acro{CPU}s. After these changes, the tests finish in about 15 minutes, which is a 60-fold speed increase and which makes the tests practically useful to developers.

In this article, we describe the testing framework of the Markdown package. In sections~\ref{sec:definition-files} through~\ref{sec:error-handling-strategies}, we describe the definition files, techniques, and strategies used in our framework. In Section~\ref{sec:implementation}, we describe the details of our implementation. In sections~\ref{sec:experiments} and~\ref{sec:results}, we describe our experiments and their results. In~Section~\ref{sec:related-work}, we discuss prior work related to our framework. We conclude in Section~\ref{sec:conclusion} by summarizing our contributions and outlining future~work.

\section{Definition files}
\label{sec:definition-files}

In the future, an \acro{AI} agent might examine the code of a \TeX{} package and identify any incorrect behavior. For the moment, regression testing requires the manual creation of many definition files that describe the expected behavior and how it should be validated.

In this section, we describe the definition files used in our framework: test files, formats, commands, and templates.

\subsection{Test files}
\label{sec:test-files}

\looseness=-1
The Markdown package converts markdown text to \TeX{} commands. To validate the conversion, our framework redefines these \TeX{} commands to produce output in the \texttt{.log} file, which we then examine.

A \emph{test file} consists of a) \TeX{} code that configures the Markdown package, b) markdown text, and c)~the expected output in the \texttt{.log} file.

As an example, \texttt{strike-through.test} tests the strike-through syntax extension:

\smallskip
\noindent
\example*[\input images/strike-through.tex]{strike-through.test}

\subsection{Formats, commands, and templates}
\label{sec:formats-commands-and-templates}

\looseness=-1
The Markdown package supports several combinations of \TeX{} formats and engines. For each \TeX{} format, there are also several ways to input markdown text. Our framework ensures that a markdown text always produces the same output.

A \emph{format} consists of one or more a) \emph{commands} that can be used to typeset documents in a \TeX{} format using different \TeX{} engines and b) \emph{templates} that specify the different ways in which markdown text can be input with the \TeX{} format.

An example format \texttt{plain} contains commands for the \hologo{pdfTeX}, \Hologo{XeTeX}, and \Hologo{LuaTeX} engines:

\smallskip
\noindent
\addtocounter{footnote}{1}%
\footnotetext{%
  The Markdown package uses Lua to parse markdown text. Whereas Lua\TeX{} can execute Lua code directly, other \TeX{} engines must use the shell of the operating system to execute Lua code. Since accessing the shell is a security risk, users must express their consent by writing \texttt{-{}-shell-escape}.%
}%
\example[%
  \addtocounter{footnote}{-2}%
  \fvset{commandchars=\\\{\}}%
]{COMMANDS.m4}

\smallskip
\noindent
The format \texttt{plain} also contains two templates. One uses the \cs{markdownInput} \TeX{} macro and the other one uses the \cs{markdownBegin} and \texttt{End} \TeX{} macros:

\medskip
\noindent
\example{input.tex.m4}

\medskip
\exampleSeparator

\medskip
\noindent
\example[\fvset{commandchars=|<>}]{verbatim.tex.m4}

\subsection{Materialized templates and commands}
\label{sec:materialized-templates-and-commands}

During testing, the texts {\small\texttt{TEST\_SETUP\_FILENAME}} and {\small\texttt{TEST\_INPUT\_FILENAME}} in templates are replaced with names of auxiliary files that contain the \TeX{} code and the markdown text parts of a test file, respectively. Also, the text {\small\texttt{undivert(TEST\_INPUT\_FILENAME)}} is replaced with the literal markdown text from the test file. After the replacement, we say that the template has been \emph{materialized}.

Here is the template \texttt{verbatim.tex.m4} from Section~\ref{sec:formats-commands-and-templates} after it has been materialized with the test file \texttt{strike-through.test} from Section~\ref{sec:test-files}:

\medskip
\noindent
\combineFiles{verbatim.tex.m4}{strike-through.test} \\[0.4em]
\example{verbatim.tex}

\medskip
\exampleSeparator

\medskip
\noindent
\example{test-setup.tex}

\medskip

After a template has been materialized, the text \texttt{TEST\_FILENAME} in all commands is replaced with the filename of the materialized template. After the replacement, the command has also been materialized.

Here are the commands \texttt{COMMANDS.m4} from Section~\ref{sec:formats-commands-and-templates} after they have been materialized:

\smallskip
\noindent
\combineFiles{COMMANDS.m4}{verbatim.tex} \\[0.8em]
\example{COMMANDS}

\smallskip

\noindent
During testing, the materialized commands are executed. Each command produces a \texttt{.log} file, which is compared to the expected output from the test file.

\section{Computational techniques}
\label{sec:computational-techniques}

While testing all combinations of test files, templates, and commands ensures comprehensive coverage of all potential configurations, it can be time-consuming.

In this section, we describe the computational techniques of multiprocessing, the batching of test files, and how they increase the speed of testing in our framework.
Furthermore, the batching of test files raises challenges with load balancing and the attribution of errors. We discuss the challenges and describe the techniques of batch size limiting and batch splitting to address them.

\subsection{Multiprocessing}
\label{sec:multiprocessing}
Whereas \TeX{} uses only a single \acro{CPU}, modern \acro{PC}s can contain several \acro{CPU}s. Therefore, we can increase the speed of testing by using \emph{multiprocessing}, where each \acro{CPU} processes a different test file:

\smallskip
\noindent
\begingroup
\centering
\input images/cpus-loaded.tex
\par
\endgroup

\smallskip
\noindent
Using $N$ \acro{CPU}s increases testing speed up to $N$ times.

\subsection{Batching of test files}
\label{sec:batching-of-test-files}
At the beginning of a document, \TeX{} initializes packages, fonts, Lua scripts, and other assets, which slows down testing:

\smallskip
\noindent
\example*[\input images/verbatim.tex]{verbatim.tex}

\smallskip

To increase the speed of testing, we can amortize the cost of initialization by materializing a template with a \emph{batch} of several test files:

\medskip
\noindent
\combineFiles{input.tex.m4}{first.test, second.test, third.test} \\
\example{input.tex}

\smallskip
\noindent
Batching $N$ test files decreases initialization cost $N$ times. How much this speeds up testing depends on the ratio between the time spent on initialization and the time spent on processing the rest of the template.

\subsection{Batch size limiting}

When we use both multiprocessing and batching with large batch sizes, most \acro{CPU}s will be unused:

\smallskip
\noindent
\begingroup
\centering
\input images/cpus-without-limiting.tex
\par
\endgroup

\smallskip
\noindent
We can \emph{limit the batch size} so that all \acro{CPU}s are used:

\medskip
\noindent
\begingroup
\centering
\input images/cpus-with-limiting.tex
\par
\endgroup

\smallskip
\noindent
\looseness=-1
Limiting the batch size increases the speed of testing, because every used \acro{CPU} has less work to do.\footnote{However, the \acro{CPU}s do more work overall, because every used \acro{CPU} has to pay the initialization cost and more \acro{CPU}s are used. Therefore, limiting the batch size increases the speed of testing but decreases energy efficiency.}

\subsection{Batch splitting}

\looseness=-1
When we test batches of test files, a \texttt{.log} file is split into sections corresponding to individual test files and compared with the expected test file outputs. However, if a fatal error occurs, the \texttt{.log} file may become malformed. To find the test file responsible for the error, we repeatedly \emph{split the batch} using binary search.

Here is how we would split a batch of test files \texttt{first.test}, \texttt{second.test}, and \texttt{third.test}, where \texttt{second.test} causes a fatal error:

\smallskip
\noindent
\begingroup
\centering
\input images/batch-splitting.tex
\par
\endgroup

\smallskip
\noindent
First, we try processing all files together but we encounter a fatal error. Therefore, we divide the files into two groups: one with \texttt{first.test} and the other with \texttt{second.test} and \texttt{third.test}. Processing these separately, we again face a fatal error in the group with \texttt{second.test} and \texttt{third.test}. We then split this group into two individual files, \texttt{second.test} and \texttt{third.test}, and we process them. The fatal error occurs with \texttt{second.test}, which we identify as the cause of the error.

When only one test file out of $N$ files in a batch causes a fatal error, batch splitting executes at most $2 (\log_2 N + 1)$ commands. This is less than or equal to $N$ for sufficiently large batch sizes $N\geq 8$. Therefore, in the presence of no more than a few fatal errors, batching is still faster than sequential processing.

\smallskip
\noindent
\begingroup
\input images/wolf-experiment
\par
\endgroup

\section{Error handling strategies}
\label{sec:error-handling-strategies}

Developers and maintainers have different needs when it comes to the handling of errors. Whereas developers need immediate feedback during the development of new features, maintainers require a~comprehensive summary of all errors when they deal with unexpected breakage.

In this section, we describe the error handling strategies of developer- and maintainer-oriented testing and updating test files. We also discuss how these strategies increase the speed of development and decrease maintenance costs.

\subsection{Developer-oriented testing}

In the practice of test-driven development, before adding a new feature, developers first write new test files that describe the expected behavior of the feature. Then, they develop the feature until all test files have passed. At the beginning, old test files will pass, whereas new test files will fail. At the end, all test files, both old and new, should pass.

In order to increase the speed of development, tests should fail fast to provide immediate feedback. Therefore, developers can configure our framework to start with the new test files, which are the most likely to fail, and stop at the first error rather than wait until all tests have finished.

For example, imagine a \TeX{} package with two test files: \texttt{first.test} and \texttt{second.test}. For simplicity, the package has only one format with one template and with three commands for the \hologo{pdfTeX}, \Hologo{XeTeX}, and \Hologo{LuaTeX} engines. Before the development of a new feature, developers add a new test file \texttt{third.test} and they run the tests with the following results:

\smallskip
\noindent
\begingroup
\centering
\input images/developer-oriented-testing
\par
\endgroup

\smallskip
\noindent
Since \texttt{third.test} was new, it was tested first and immediately failed, providing immediate feedback to developers. This is how we test all updates submitted to the GitHub repository of the Markdown package.

\subsection{Maintainer-oriented testing}

Tests can fail not just during the development of new features but also during maintenance. These errors are often caused by changes to external dependencies such as \TeX{} engines, formats, and packages.

In order to decrease maintenance costs, tests should provide comprehensive feedback. Therefore, maintainers can configure our framework to always process all test files and produce a helpful summary of all errors.

Continuing the example from the previous section, developers finish the new feature and release an updated version of their package on \acro{CTAN}. However, after a month, the tests fail with the following results:
\looseness=-1

\smallskip
\noindent
\begingroup
\centering
\input images/maintainer-oriented-testing
\par
\endgroup

\smallskip
\noindent
At a glance, the summary shows that the errors are related to the \Hologo{XeTeX} engine. This is how we test the Markdown package every week.

\subsection{Updating test files}
\label{sec:updating-test-files}

Although test-driven development is well-suited to adding new features, fundamental changes to the code may require that existing test files are updated as well. Furthermore, writing test files before the development of a feature can be difficult, especially for complex features with incomplete requirements.

In order to increase the speed of development, developers can configure our framework to \emph{update test files} instead of failing. Developers can make fundamental changes and our framework will update the expected outputs in test files to match the actual output. Furthermore, developers can also develop a~feature, write partial test files for the feature that contain only the \TeX{} code and markdown text, and use our framework to fill in the expected output. Then, developers can review the changes and determine whether they are correct.

Our framework will update a test file only if all templates and commands produce consistent outputs. In the example from the previous section, the command for the \Hologo{XeTeX} engine failed for all test files, whereas the other commands did not. Therefore, our framework would not update any test files and fail.

\section{Implementation}
\label{sec:implementation}

Before Markdown 3.0.0, our framework was implemented by a Bash script \texttt{test.sh}; see Listing~\ref{lst:test.sh}.

\begin{listing*}
\bigExample*[\input images/test.tex]{test.sh}
\caption{The shell script \texttt{test.sh} that implemented the testing framework of the Markdown package before version 3.0.0. For each test file, \texttt{test.sh} a) materializes templates in a temporary directory, b) executes materialized commands, c) compares the \texttt{.log} file against the expected output, and d) optionally updates the test file.}
\label{lst:test.sh}
\vspace{-2pt} % avoid "float too large" warning
\end{listing*}

At first, \texttt{test.sh} processed test files sequentially and did not use the computational techniques from Section~\ref{sec:computational-techniques} to increase the speed of testing. Since Markdown 2.4.0, we used the \acro{GNU} Parallel command-line tool~\cite{tange2011gnu} to implement multiprocessing:
\begin{verbatim}
$ find -name '*.test' | parallel ./test.sh
\end{verbatim}
Out of the error handling strategies from Section~\ref{sec:error-handling-strategies}, \texttt{test.sh} could only update test files.

While Bash is convenient for simple programs, more complicated programs are better written in a~more expressive language. In Markdown 3.0.0, we rewrote our framework from Bash to Python~3~\cite{novotny2023implement}.

Out of the techniques and strategies from Sections~\ref{sec:computational-techniques} and \ref{sec:error-handling-strategies}, the higher expressiveness of Python allowed us to implement the batching of test files, developer- and maintainer-oriented testing, and batch splitting. Furthermore, Python's built-in support for multiprocessing allowed us to stop using \acro{GNU} Parallel and implement batch size limiting.

\section{Experiments}
\label{sec:experiments}

In this section, we describe our experiments with multiprocessing and batching. In our experiments, we aimed to answer the following research questions:
\begin{enumerate}
\item What is the speed benefit of multiprocessing?
\item What is the speed benefit of batching test files?
\item What is the speed benefit of batch size limiting?
\end{enumerate}
\looseness=-1
To answer the questions, we tested the Markdown package with different \acro{CPU} counts and batch sizes:
\begin{itemize}
\item Numbers of \acro{CPU}s: 1, 2, 4, 8, 16, and 32
\item Batch sizes: 1, 2, 4, 8, \ldots, 256, 512, and 1024
\end{itemize}
To ensure reliability of our findings, we repeated each test configuration five times and we measured the median testing time to control for sample variance. Our experimental code is available online.~\cite{starynovotny2023measure}

We tested the Markdown package at Git commit \texttt{7613632} from August 21, 2023. At this commit, the Markdown package contained 783 test files. For each test file, 14 commands were materialized and executed: four for plain \TeX, four for \LaTeX, and six for \hologo{ConTeXt} MkIV. Therefore, \TeX{} formats were initialized up to $14\cdot 783 = 10{,}962$ times during testing.

To show the speed benefit of batch size limiting, we deactivated it in our experiments, thereby highlighting the speed reduction caused by its absence.

We ran the experiments for 33 days on a shared \GNU/Linux server with 400~\acro{GB} of \acro{RAM} and 80 \acro{CPU}s, each at 2.1~GHz.

\begin{figure}[b!]
\input images/speed-tests.tex
\caption{The median testing times for different numbers of \acro{CPU}s and batch sizes}
\label{fig:results}
\end{figure}

\section{Results}
\label{sec:results}

Figure~\ref{fig:results} shows the results of our experiments. In this section, we discuss the results and how they relate to the three research questions from previous section.

\subsection{Multiprocessing}
With batch size 1, the testing speed scales almost linearly with the number of \acro{CPU}s, as we would expect: Whereas with 1~\acro{CPU}, the median testing time is 27~hours and 2~minutes, it is only 1~hour and 6~minutes with 32~\acro{CPU}s (about 24-fold speed-up).\footnote{%
The reason that we did not achieve the theoretical 32-fold speedup is likely tasks from other users on our server.%
}

\subsection{Batching of test files}
With 1~\acro{CPU}, the testing speed also scales almost linearly with the batch size, up to a point. Whereas with batch size 1, the median testing time is 27~hours and 2~minutes, it is only 4~hours and 30~minutes with batch size 8 (about 6-fold speed-up), and 1~hour and 20~minutes with batch size 512 (about 21-fold speed-up). This indicates that initialization dominates the testing time.

To better understand the relationship between the initialization and the testing time, we can solve the following series of equations:
\begin{align*}
    14\cdot(783\cdot(X + Y)) &= \text{27 hours and 2 minutes} \\
    14\cdot(X + 783\cdot Y) &= \text{1 hour and 16 minutes}
\end{align*}
On the left-hand side of the equations, the variable $X$ stands for the mean time that it takes to initialize a \TeX{} format and the variable $Y$ stands for the mean time that it takes to process the markdown text from a single test file. On the right-hand side of the equations are the median testing times with 1~\acro{CPU} and batch sizes 1 (above) and 1024 (below).

The solution shows that whereas it takes a full $X\approx\text{8.47 seconds}$ to initialize a \TeX{} format, it takes only $Y\approx\text{0.41 seconds}$ to process a markdown text. In other words, without batching, 95\% of time is spent on initialization and only 5\% on actual testing; with batching, up to 97\% of time is spent on testing.

\subsection{Batch size limiting}
The speed improvements from multiprocessing and batching are additive, up to a point. Whereas with 1~\acro{CPU} and batch size 1, the median testing time is 27~hours and 2~minutes, it is only 10~minutes with 32~\acro{CPU}s and batch size 32 (about 161-fold speed-up).

When the number of \acro{CPU}s multiplied by the batch size exceeds the number of test files (783), we cannot use all \acro{CPU}s and the testing speed decreases. Whereas with 32~\acro{CPU}s and batch size 16, the median testing time is only 11~minutes, it is 1~hour and 24~minutes with the same number of \acro{CPU}s and batch size 1024 (about 8-fold slow-down). Our framework prevents this effect by limiting the batch size.

\section{Related work}
\label{sec:related-work}

Besides our framework, there exist other frameworks for regression testing of \TeX{} packages. Furthermore, the computational techniques in our framework are often adapted from previous work in other fields.

In this section, we discuss the regression testing framework of the l3build package management system and we compare it with our framework. Furthermore, we discuss the origin of the batching of test files and batch splitting.

\subsection{The l3build package}
The l3build package~\cite{mittelbach2014l3build, wright2015automating, wright2022l3build, latex2023l3build} provides a comprehensive system for \TeX{} package management that also includes a regression testing framework.

Whereas our framework is written in Python, l3build is written in Lua. Each language presents its own set of strengths and weaknesses. On one hand, every modern installation of \TeX{} includes a Lua interpreter, which makes l3build more accessible for \TeX{} users compared to our framework. On the other hand, unlike Python, Lua has no built-in support for multiprocessing. Therefore, l3build users must use external tools like \acro{GNU}~Parallel to use multiple \acro{CPU}s for testing, whereas our framework can use multiple \acro{CPU}s out of the box.

In our framework, each test file is designed to hold a single self-contained test that avoids modifying the global state. This design allows for straightforward grouping of tests into batches, where each test is isolated from others using \TeX{} groups, as we discussed in Section~\ref{sec:batching-of-test-files}.

In l3build, a single test file may contain multiple tests. These tests can be interdependent, creating a~challenge in separating them from their files. Additionally, these tests might change the global state, which poses a risk of unexpected conflicts when tests are grouped into batches. Due to these complexities, l3build does not support the batching of tests. Nonetheless, the practice of including multiple tests in a single test file can be seen as a form of manual batching that amortizes the cost of initialization.

Both our framework and l3build support the updating of test files~\cite[Section~2.7]{latex2023l3build}. This allows developers to automatically generate parts of test files when they make fundamental changes to the code or when they develop complex new features, as discussed in Section~\ref{sec:updating-test-files}.

\subsection{Batching and batch splitting}
The techniques of batching and batch splitting were perhaps first used with \TeX{} in the \ARQMath\ competitions in large-scale indexing of math formulae.

In the first \ARQMath\ competition, the \acro{MIRMU} team used the \LaTeX\acro{ML} tool to convert \TeX{} formulae to \acro{XML}~\cite[Section~2.2]{novotny2020three}. Due to speed issues when processing each formula separately, \acro{MIRMU} processed them in batches. However, a single error would cause the loss of an entire batch. Therefore, \acro{MIRMU} used batch splitting to recover correct formulae after an error~\cite{novotny2020arqmath}.
% The combination of batching and batch splitting allowed \acro{MIRMU} to convert the formulae quickly and losslessly.

In the third \ARQMath\ competition, the organizers used the same techniques to provide math formulae in the \acro{XML} format to all participants.

\section{Conclusion}
\label{sec:conclusion}

Larger \TeX{} packages commonly use regression tests to ensure code integrity over time.
In this study, we explored techniques for speeding up the regression testing of \TeX{} packages. We have shown that batching test files can improve the testing efficiency from 5\% to 97\%. We have also shown that multiprocessing on 32 \acro{CPU}s combined with the batching of test files increases testing speed up to 161 times.

The lessons learned from our work are as follows:

\begin{itemize}
\item Whereas \TeX{} uses only a single \acro{CPU}, modern \acro{PC}s can contain several \acro{CPU}s. Multiprocessing can increase testing speed by using these \acro{CPU}s.
\item Writing small isolated test files is convenient for authors but carries a high initialization cost during testing. For \TeX{} packages such as the Markdown package, where tests are easy to isolate, the batching of test files can be used to recover the initialization cost.
\item Whereas developers need tests to fail as fast as possible, maintainers benefit from a comprehensive summary of all errors.
\end{itemize}

We hope that our practical lessons will improve the regression testing practices used in the development and maintenance of \TeX{} packages. With fast regression testing, developers can quickly introduce new features, while maintainers can proactively address emerging issues before they affect users.

\medskip
\noindent
\input images/wolf-f1

\section*{Disclaimer}
No wolves were harmed in the making of this article.

\section*{Acknowledgements}
We would like to gratefully acknowledge the Natural Language Processing Center at the Faculty of Informatics, Masaryk University, Brno, who kindly provided computational resources for our experiments.

We also wish to extend our special thanks to Joseph Wright, Ben Frank,
Barbara Beeton,
and Karl Berry for their invaluable insights and thorough review of our work. Their expertise and thoughtful feedback have been instrumental in shaping the final manuscript.

\section*{Donation request}
Despite our breakthroughs in testing speed, additional computational
resources would greatly accelerate the development and maintenance
of the Markdown package for \TeX.

We graciously invite donations of GitHub self-hosted runners,
particularly those hosted on \acro{GNU}\slash Linux servers with at
least 16 \acro{GB} of \acro{RAM} and 12 \acro{CPU}s. For more
information, please contact us by email. Donors will be acknowledged in
the project documentation, with the option to remain anonymous upon
request.

\SetBibJustification{\raggedright \advance\itemsep by 2pt plus1pt minus1pt }
\bibliographystyle{tugboat}
\begingroup
\gappto{\UrlBreaks}{\UrlOrds}
%\RaggedRight % let's not hyphenate titles
%\small
\bibliography{starynovotny-testing}
\endgroup

\bigskip
\makesignature
\end{document}
